{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "622d0ad1",
   "metadata": {},
   "source": [
    "# Classifying recorded audio files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "319bd622",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function tensorflow.python.eager.polymorphic_function.polymorphic_function.run_functions_eagerly(run_eagerly)>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from scipy import signal\n",
    "from scipy.io import wavfile\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from PIL import Image\n",
    "import matplotlib\n",
    "import random\n",
    "\n",
    "import scipy\n",
    "import librosa\n",
    "import matplotlib.image\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "\n",
    "\n",
    "tf.config.run_functions_eagerly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c0a2cfde",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c8b14e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the saved best model ( see RNN.ipynb)\n",
    "model = tf.keras.models.load_model('./model3.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "afd625c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "REFERENCE_LENGTH = 14550.0\n",
    "\n",
    "spectrograms = []\n",
    "\n",
    "test_labels = [\"go\", \"down\", \"left\", \"no\", \"off\", \"on\", \"right\", \"stop\", \"up\", \"yes\"]\n",
    "\n",
    "for el in test_labels:\n",
    "    \n",
    "    path = \"./recorded_audio_wav/s_\"+el+\".wav\"\n",
    "\n",
    "    # loading audio file\n",
    "    song, fs = librosa.load(path)\n",
    "\n",
    "    # stretching to Time duration equivalent to 64 pixels\n",
    "    song_stretched = librosa.effects.time_stretch(song, song.shape[0]/REFERENCE_LENGTH)\n",
    "    frequencies, times, spectrogram = signal.spectrogram(song_stretched, fs)\n",
    "\n",
    "    # spectorgram path\n",
    "    arr = path.split(\"/\")\n",
    "    spectrogram_path = \"spectrograms/\" + arr[1] + \"/\" + arr[2][:-4] + \".png\"\n",
    "\n",
    "    # scaling spectrograms to 64 x 64 pixels & 0 - 255 integers\n",
    "    spectrogram = spectrogram[0:64, 0:64]\n",
    "    spectrogram = (np.round(spectrogram/(spectrogram.max()/255.0))).astype(np.uint8)\n",
    "    \n",
    "    spectrograms.append(spectrogram)\n",
    "    \n",
    "# additional audio files - stop and right and yes (recorded by Patryk)\n",
    "path = \"./recorded_audio_wav/p_stop.wav\"\n",
    "\n",
    "# loading audio file\n",
    "song, fs = librosa.load(path)\n",
    "\n",
    "# stretching to Time duration equivalent to 64 pixels\n",
    "song_stretched = librosa.effects.time_stretch(song, song.shape[0]/REFERENCE_LENGTH)\n",
    "frequencies, times, spectrogram = signal.spectrogram(song_stretched, fs)\n",
    "\n",
    "# spectorgram path\n",
    "arr = path.split(\"/\")\n",
    "spectrogram_path = \"spectrograms/\" + arr[1] + \"/\" + arr[2][:-4] + \".png\"\n",
    "\n",
    "# scaling spectrograms to 64 x 64 pixels & 0 - 255 integers\n",
    "spectrogram = spectrogram[0:64, 0:64]\n",
    "spectrogram = (np.round(spectrogram/(spectrogram.max()/255.0))).astype(np.uint8)\n",
    "\n",
    "spectrograms.append(spectrogram)\n",
    "\n",
    "test_labels.append(\"stop\")\n",
    "\n",
    "# #####    2nd\n",
    "path = \"./recorded_audio_wav/p_right.wav\"\n",
    "\n",
    "# loading audio file\n",
    "song, fs = librosa.load(path)\n",
    "\n",
    "# stretching to Time duration equivalent to 64 pixels\n",
    "song_stretched = librosa.effects.time_stretch(song, song.shape[0]/REFERENCE_LENGTH)\n",
    "frequencies, times, spectrogram = signal.spectrogram(song_stretched, fs)\n",
    "\n",
    "# spectorgram path\n",
    "arr = path.split(\"/\")\n",
    "spectrogram_path = \"spectrograms/\" + arr[1] + \"/\" + arr[2][:-4] + \".png\"\n",
    "\n",
    "# scaling spectrograms to 64 x 64 pixels & 0 - 255 integers\n",
    "spectrogram = spectrogram[0:64, 0:64]\n",
    "spectrogram = (np.round(spectrogram/(spectrogram.max()/255.0))).astype(np.uint8)\n",
    "\n",
    "spectrograms.append(spectrogram)\n",
    "\n",
    "test_labels.append(\"right\")\n",
    "\n",
    "# #####    3rd\n",
    "path = \"./recorded_audio_wav/p_yes.wav\"\n",
    "\n",
    "# loading audio file\n",
    "song, fs = librosa.load(path)\n",
    "\n",
    "# stretching to Time duration equivalent to 64 pixels\n",
    "song_stretched = librosa.effects.time_stretch(song, song.shape[0]/REFERENCE_LENGTH)\n",
    "frequencies, times, spectrogram = signal.spectrogram(song_stretched, fs)\n",
    "\n",
    "# spectorgram path\n",
    "arr = path.split(\"/\")\n",
    "spectrogram_path = \"spectrograms/\" + arr[1] + \"/\" + arr[2][:-4] + \".png\"\n",
    "\n",
    "# scaling spectrograms to 64 x 64 pixels & 0 - 255 integers\n",
    "spectrogram = spectrogram[0:64, 0:64]\n",
    "spectrogram = (np.round(spectrogram/(spectrogram.max()/255.0))).astype(np.uint8)\n",
    "\n",
    "spectrograms.append(spectrogram)\n",
    "\n",
    "test_labels.append(\"yes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "b26d600a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dummies2 = pd.get_dummies(test_labels)\n",
    "\n",
    "dataset_test = tf.data.Dataset.from_tensor_slices((spectrograms, dummies2)).batch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "3fb2b4b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/13 [==============================] - 0s 7ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[2.52355152e-04, 1.02243954e-02, 4.63765446e-06, 4.51811843e-07,\n",
       "        8.57586265e-01, 1.30661756e-01, 1.08830291e-05, 3.19388841e-04,\n",
       "        9.39792488e-04, 9.26538273e-12],\n",
       "       [1.60823110e-02, 9.82429385e-01, 3.38265913e-08, 2.38241100e-05,\n",
       "        2.92846118e-04, 1.00447750e-03, 1.73617900e-05, 2.24947107e-05,\n",
       "        1.27327861e-04, 7.95751919e-12],\n",
       "       [4.05616277e-07, 7.82548886e-06, 3.28175016e-02, 2.99161329e-04,\n",
       "        8.49080237e-08, 8.99308290e-08, 8.57531846e-01, 3.34665237e-05,\n",
       "        9.73415598e-02, 1.19680334e-02],\n",
       "       [3.20640829e-05, 7.03668714e-01, 7.89307614e-06, 1.13613217e-03,\n",
       "        9.94015019e-04, 5.33086350e-05, 1.35897544e-05, 1.09773675e-04,\n",
       "        2.93984443e-01, 2.77075113e-10],\n",
       "       [6.69290777e-04, 2.47741537e-03, 3.62146558e-04, 1.89485319e-03,\n",
       "        7.77622452e-03, 1.69263060e-07, 4.36469636e-05, 3.41994502e-03,\n",
       "        9.83353853e-01, 2.41673820e-06],\n",
       "       [3.74716730e-03, 8.25264899e-04, 8.67528899e-04, 1.27036415e-07,\n",
       "        2.84195617e-02, 9.52987731e-01, 4.82554460e-04, 4.61026607e-03,\n",
       "        8.05920549e-03, 5.80471635e-07],\n",
       "       [1.62326265e-04, 8.22351407e-03, 2.17150882e-04, 5.58668445e-09,\n",
       "        3.80558938e-01, 4.80951704e-02, 4.03040648e-03, 4.83202152e-02,\n",
       "        5.10391772e-01, 5.76342586e-07],\n",
       "       [1.59826214e-08, 3.37020814e-04, 2.19242880e-03, 6.61018657e-06,\n",
       "        1.99184194e-03, 9.35089606e-09, 7.82009331e-04, 5.89322444e-05,\n",
       "        9.94631171e-01, 3.94005673e-09],\n",
       "       [4.00311791e-07, 1.59530115e-04, 2.08759266e-05, 1.98593426e-07,\n",
       "        2.31580329e-04, 2.76419485e-07, 1.58915356e-07, 2.43150610e-02,\n",
       "        9.75271881e-01, 2.21472597e-11],\n",
       "       [2.20211642e-03, 1.24542005e-01, 7.28066385e-01, 1.21092203e-03,\n",
       "        2.27422992e-04, 9.14869872e-07, 3.12780612e-04, 2.37295823e-03,\n",
       "        2.65459530e-03, 1.38409823e-01],\n",
       "       [2.05028829e-08, 5.99611294e-09, 2.30083067e-04, 8.32925728e-09,\n",
       "        1.87604037e-05, 8.73604108e-07, 2.15250353e-07, 9.17181969e-01,\n",
       "        8.25677887e-02, 2.40709454e-07],\n",
       "       [5.03503701e-15, 2.75959561e-10, 2.43437593e-04, 1.47719933e-12,\n",
       "        1.32413538e-08, 7.94798394e-11, 9.99756038e-01, 9.56096140e-12,\n",
       "        4.45216898e-07, 7.72927469e-12],\n",
       "       [4.39763227e-07, 7.79730502e-09, 8.27974498e-01, 1.68788722e-06,\n",
       "        3.56156216e-09, 7.84507806e-11, 2.07802485e-04, 1.39724204e-06,\n",
       "        6.52150993e-06, 1.71807781e-01]], dtype=float32)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(dataset_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "0381a07f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/13 [==============================] - 0s 7ms/step\n"
     ]
    }
   ],
   "source": [
    "pred = model.predict(dataset_test)\n",
    "\n",
    "aux = np.argmax(pred, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "2c9ba493",
   "metadata": {},
   "outputs": [],
   "source": [
    "NumbersToWordsMap = {9: \"yes\", 3:\"no\",8:\"up\",0:\"down\",2:\"left\",6:\"right\",\n",
    "                     5:\"on\",4:\"off\",7:\"stop\",1:\"go\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "81f0ac2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['off', 'go', 'right', 'go', 'up', 'on', 'up', 'up', 'up', 'left',\n",
       "       'stop', 'right', 'left'], dtype='<U5')"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_labels = np.vectorize(NumbersToWordsMap.get)(aux)\n",
    "\n",
    "predicted_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "e31da1cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['go',\n",
       " 'down',\n",
       " 'left',\n",
       " 'no',\n",
       " 'off',\n",
       " 'on',\n",
       " 'right',\n",
       " 'stop',\n",
       " 'up',\n",
       " 'yes',\n",
       " 'stop',\n",
       " 'right',\n",
       " 'yes']"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3be6d98",
   "metadata": {},
   "source": [
    "# Conclusions\n",
    "\n",
    "Perhaps 10 audio files recorded by another volunteer were too silent and that is why the model classified well only 2 out of 10 audio files coming from this person.\n",
    "\n",
    "After adding 2 audio files ( \"stop\" and \"right\"), the model predicted both true labels with success."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "207f91cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video src=\"./recorded_audio_mp4/p_stop.mp4\" controls  >\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Video\n",
    "\n",
    "Video(\"./recorded_audio_mp4/p_stop.mp4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c238a080",
   "metadata": {},
   "source": [
    "Even if there is a noise (I recorded it in Warsaw Swietokrzyska metro station, the model could discern the spoken word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "cc544bc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video src=\"./recorded_audio_mp4/p_right.mp4\" controls  >\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Video(\"./recorded_audio_mp4/p_right.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "55b2e235",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video src=\"./recorded_audio_mp4/p_yes.mp4\" controls  >\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Video(\"./recorded_audio_mp4/p_yes.mp4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6056dd45",
   "metadata": {},
   "source": [
    "'Yes' was misclassified as 'left'.\n",
    "\n",
    "According to confusion matrix, 'left' was the most common wrong answer when the true label was 'yes' - perhaps it is a matter of accent.\n",
    "\n",
    "Nevertheless, the results on real-life recorded audio files correspond to expectations from confusion matrix. Still, it is recommended to provide audible audio files with a good english accent."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
